### Big O Notation üôáüèª
<pre>
‚ñ∂Ô∏è It's hard to pin down the exact runtime of an algorithm. It depends on the speed of the processor,
what else the computer is running, etc. To standardize talking about how much time and how much space 
is required for an algorithm to run, big O was invented. 

‚ñ∂Ô∏è Big O notation is a simple method to represent  the time complexity

‚ñ∂Ô∏è Big O notation is a theoretical concept used to measure how running time or space requirements for a 
program grows as input size increases without actually executing the algorithm. It gives an upper bound
of the complexity in the worst case.

‚ñ∂Ô∏è If we were measuring our runtime directly, we could express our speed in seconds. Since we're measuring 
how quickly our runtime grows, we need to express our speed in terms of something else. With Big O notation,
we use the size of the input, which we call "n".

‚ñ∂Ô∏è When we are calculating the big O complexity of some algorithm, we should not consider the constants
</pre>
